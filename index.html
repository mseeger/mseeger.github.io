<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Matthias Seeger</title>
</head>
<body>
<div id="layout-content">
<div id="toptitle">
<h1>Matthias Seeger</h1>
</div>
<table class="imgtable"><tr><td>
<img src="files/seeger_photo_small_portr.jpg" alt="alt text" width="347px" height="384px" />&nbsp;</td>
<td align="left"><p><b>Principal Machine Learning Scientist at Amazon</b> <br />
<br />
Contact: <i>mseeger</i> [@] gmail [DOT] com, <i>matthis</i> [@] amazon [DOT] com <br />
<br />
<a href="https://scholar.google.com/citations?user=V-lc8A8AAAAJ&amp;hl=en">[Google Scholar]</a> <br />
<a href="https://dblp.org/pers/hd/s/Seeger:Matthias_W=">[dblp]</a> <br />
<a href="https://www.linkedin.com/in/matthias-seeger-3010b765/">[LinkedIn]</a></p>
</td></tr></table>
<div class="infoblock">
<div class="blocktitle">Short Bio</div>
<div class="blockcontent">
<p>Matthias W. Seeger received a Ph.D. from the <a href="https://www.ed.ac.uk/informatics/">School of Informatics</a>, Edinburgh university, UK, in 2003 (advisor <a href="https://homepages.inf.ed.ac.uk/ckiw/">Christopher Williams</a>). He was a research fellow with <a href="https://people.eecs.berkeley.edu/~jordan/">Michael Jordan</a> and <a href="https://www.stat.berkeley.edu/~bartlett/">Peter Bartlett</a>, University of California at Berkeley, from 2003, and with <a href="https://www.is.mpg.de/person/bs">Bernhard Schoelkopf</a>, Max Planck Institute for Intelligent Systems, Tuebingen, Germany, from 2005. He led a research group at the University of Saarbruecken, Germany, from 2008, and was assistant professor at the <a href="https://www.epfl.ch/schools/ic/">Ecole Polytechnique Federale de Lausanne</a> from fall 2010. He joined Amazon as machine learning scientist in 2014.</p>
</div></div>
<div class="infoblock">
<div class="blocktitle">Research Interests</div>
<div class="blockcontent">
<p>My interests center around Bayesian learning and decision making with probabilistic models, from gaining understanding to making it work in large scale practice. I have been working on theory and practice of Gaussian processes, scalable variational approximate inference algorithms, and Bayesian compressed sensing. More recently, I worked on demand forecasting and Bayesian optimization (hyperparameter tuning, AutoML).</p>
</div></div>
<div class="infoblock">
<div class="blocktitle">Publications</div>
<div class="blockcontent">
<p><b>Conference:</b></p>
<ul>
<li><p>(2018) V. Perrone, R. Jenatton, M. Seeger, C. Archambeau. Scalable Hyperparameter Transfer Learning. <i>Neural Information Processing Systems 31: 6846-6856</i>. <a href="http://papers.nips.cc/paper/7917-scalable-hyperparameter-transfer-learning.pdf">[pdf]</a></p>
</li>
<li><p>(2018) S. Rangapuram, M. Seeger, J. Gasthaus, L. Stella, Y. Wang, T. Januschowski. Deep State Space Models for Time Series Forecasting. <i>Neural Information Processing Systems 31: 7796-7805</i>. <a href="http://papers.nips.cc/paper/8004-deep-state-space-models-for-time-series-forecasting.pdf">[pdf]</a></p>
</li>
<li><p>(2017) J. Boese, V. Flunkert, J. Gasthaus, T. Januschowski, D. Lange, D. Salinas, S. Schelter, M. Seeger, Y. Wang. Probabilistic Demand Forecasting at Scale. PVLDB 10(12): 1694-1705/. <a href="http://www.vldb.org/pvldb/vol10/p1694-schelter.pdf">[pdf]</a></p>
</li>
<li><p>(2017) R. Jenatton, C. Archambeau, J. Gonzalez, M.Seeger. Bayesian Optimization with Tree-structured Dependencies. <i>International Conference on Machine Learning 34: 1655-1664</i>. <a href="http://proceedings.mlr.press/v70/jenatton17a/jenatton17a.pdf">[pdf]</a></p>
</li>
<li><p>(2016) M. Seeger, D. Salinas, V. Flunkert. Bayesian Intermittent Demand Forecasting for Large Inventories. <b>Oral</b> at <i>Neural Information Processing Systems 29: 4646-4654</i>. <a href="http://papers.nips.cc/paper/6313-bayesian-intermittent-demand-forecasting-for-large-inventories.pdf">[pdf]</a></p>
</li>
<li><p>(2015) Y. J. Ko, M. Seeger. Expectation Propagation for Rectified Linear Poisson Regression. <i>Asian Conference on Machine Learning 7</i>. <a href="https://infoscience.epfl.ch/record/214372/files/Ko49.pdf">[pdf]</a></p>
</li>
<li><p>(2014) M. Khan, Y. J. Ko, M. Seeger. Scalable Collaborative Bayesian Preference Learning. <i>Artificial Intelligence and Statistics 17: 475-483</i>. <a href="https://infoscience.epfl.ch/record/196605/files/khan14.pdf">[pdf]</a></p>
</li>
<li><p>(2013) M. Khan, A. Aravkin, M. Friedlander, M. Seeger. Fast Dual Variational Inference for Non-Conjugate Latent Gaussian Models. <i>International Conference on Machine Learning 30</i>. <a href="https://infoscience.epfl.ch/record/186800/files/final_paper_967.pdf">[pdf]</a></p>
</li>
<li><p>(2012) M. Seeger, G. Bouchard. Fast Variational Bayesian Inference for Non-Conjugate Matrix Factorization Models. <i>Artificial Intelligence and Statistics 15</i>. <a href="https://infoscience.epfl.ch/record/174931/files/aistats2012camera-ready_submitted.pdf">[pdf]</a></p>
</li>
<li><p>(2012) Y. J. Ko, M. Seeger. Large Scale Variational Bayesian Inference for Structured Scale Mixture Models. <i>International Conference on Machine Learning 29</i>. <a href="https://infoscience.epfl.ch/record/177211/files/icml12_struct_sparse.pdf">[pdf]</a></p>
</li>
<li><p>(2011) M. Seeger, H. Nickisch. Fast Convergent Algorithms for Expectation Propagation Approximate Bayesian Inference. <i>Artificial Intelligence and Statistics 14</i>. <a href="http://proceedings.mlr.press/v15/seeger11a/seeger11a.pdf">[pdf]</a></p>
</li>
<li><p>(2010) M. Seeger. Speeding up Magnetic Resonance Image Acquisition by Bayesian Multi-Slice Adaptive Compressed Sensing. <i>Neural Information Processing Systems 23: 1633-1641</i>. <a href="https://papers.nips.cc/paper/3712-speeding-up-magnetic-resonance-image-acquisition-by-bayesian-multi-slice-adaptive-compressed-sensing">[pdf]</a></p>
</li>
<li><p>(2010) M. Seeger. Gaussian Covariance and Scalable Variational Inference. <i>International Conference on Machine Learning 27</i>. <a href="https://infoscience.epfl.ch/record/161304/files/icml10_seeger.pdf">[pdf]</a></p>
</li>
<li><p>(2010) N. Srinivas, A. Krause, S. Kakade, M. Seeger. Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design. <i>International Conference on Machine Learning 27</i>. <a href="https://infoscience.epfl.ch/record/161305/files/gp_bandit_final_icml10.pdf">[pdf]</a></p>
</li>
<li><p>(2009) M. Seeger. Sparse Linear Models: Variational Approximate Inference and Bayesian Experimental Design. <i>Journal of Physics: Conference Series, 197(012001)</i>. <a href="https://infoscience.epfl.ch/record/161307/files/jpconf9_197_012001.pdf">[pdf]</a></p>
</li>
<li><p>(2009) H. Nickisch, M. Seeger. Convex Variational Bayesian Inference for Large Scale Generalized Linear Models. <i>International Conference on Machine Learning 26: 761-768</i>. <a href="https://infoscience.epfl.ch/record/161308/files/icml09_nickisch_seeger.pdf">[pdf]</a></p>
</li>
<li><p>(2009) M. Seeger, H. Nickisch, R. Pohmann, B. Schoelkopf. Bayesian Experimental Design of Magnetic Resonance Imaging Sequences. <i>Neural Information Processing Systems 21: 1441-1448</i>. <a href="https://papers.nips.cc/paper/3558-bayesian-experimental-design-of-magnetic-resonance-imaging-sequences.pdf">[pdf]</a></p>
</li>
<li><p>(2009) D. Nguyen-Tuong, J. Peters, M. Seeger. Local Gaussian Process Regression for Real Time Online Model Learning. <i>Neural Information Processing Systems 21</i>. <a href="https://papers.nips.cc/paper/3403-local-gaussian-process-regression-for-real-time-online-model-learning.pdf">[pdf]</a></p>
</li>
<li><p>(2008) M. Seeger, H. Nickisch. Compressed Sensing and Bayesian Experimental Design. <i>International Conference on Machine Learning 25</i>. <a href="https://infoscience.epfl.ch/record/161310/files/compr_sense_revised.pdf">[pdf]</a></p>
</li>
<li><p>(2008) S. Gerwinn, J. Macke, M. Seeger, M. Bethge. Bayesian Inference for Spiking Neuron Models with a Sparsity Prior. <i>Neural Information Processing Systems 21: 529-536</i>. <a href="https://papers.nips.cc/paper/3300-bayesian-inference-for-spiking-neuron-models-with-a-sparsity-prior.pdf">[pdf]</a></p>
</li>
<li><p>(2007) M. Seeger. Cross-Validation Optimization for Large Scale Hierarchical Classification Kernel Methods. <i>Neural Information Processing Systems 20: 1233-1240</i>. <a href="https://papers.nips.cc/paper/3044-cross-validation-optimization-for-large-scale-hierarchical-classification-kernel-methods.pdf">[pdf]</a></p>
</li>
<li><p>(2007) M. Seeger, F. Steinke, K. Tsuda. Bayesian Inference and Optimal Design in the Sparse Linear Model. <i>Artificial Intelligence and Statistics 11</i>. <a href="https://infoscience.epfl.ch/record/161313/files/aistats07.pdf">[pdf]</a></p>
</li>
<li><p>(2007) M. Seeger, S. Gerwinn, M. Bethge. Bayesian Inference for Sparse Generalized Linear Models. <i>European Conference on Machine Learning 2007: 298-309</i>. <a href="https://infoscience.epfl.ch/record/161313/files/aistats07.pdf">[pdf]</a></p>
</li>
<li><p>(2006) S. Kakade, M. Seeger, D. Foster. Worst-Case Bounds for Gaussian Process Models. <i>Neural Information Processing Systems 19</i>. <a href="https://papers.nips.cc/paper/2798-worst-case-bounds-for-gaussian-process-models.pdf">[pdf]</a></p>
</li>
<li><p>(2006) Y. Shen, A. Ng, M. Seeger. Fast Gaussian Process Regression Using KD-Trees. <i>Neural Information Processing Systems 19</i>. <a href="https://papers.nips.cc/paper/2835-fast-gaussian-process-regression-using-kd-trees.pdf">[pdf]</a></p>
</li>
<li><p>(2005) Y.-W. Teh, M. Seeger, M. Jordan. Semiparametric Latent Factor Models. <i>Artificial Intelligence and Statistics 10</i>. <a href="https://infoscience.epfl.ch/record/161317/files/aistats05.pdf">[pdf]</a></p>
</li>
<li><p>(2003) N. Lawrence, M. Seeger, R. Herbrich. Fast Sparse Gaussian Process Methods: The Informative Vector Machine. <i>Neural Information Processing Systems 16: 609-616</i>. <a href="https://papers.nips.cc/paper/2240-fast-sparse-gaussian-process-methods-the-informative-vector-machine.pdf">[pdf]</a></p>
</li>
<li><p>(2003) M. Seeger, C. Williams, N. Lawrence. Fast Forward Selection to Speed Up Sparse Gaussian Process Regression. <i>Artificial Intelligence and Statistics 9</i>. <a href="https://infoscience.epfl.ch/record/161318/files/aistats03-final.pdf">[pdf]</a></p>
</li>
<li><p>(2002) M. Seeger. Covariance Kernels from Bayesian Generative Models. <i>Neural Information Processing Systems 15: 905-912</i>. <a href="https://papers.nips.cc/paper/2133-covariance-kernels-from-bayesian-generative-models.pdf">[pdf]</a></p>
</li>
<li><p>(2001) C. Williams, M. Seeger. Using the Nystroem Method to Speed Up Kernel Machines. <i>Neural Information Processing Systems 14: 682-688</i>. <a href="https://papers.nips.cc/paper/1866-using-the-nystrom-method-to-speed-up-kernel-machines.pdf">[pdf]</a></p>
</li>
<li><p>(2001) M. Seeger, J. Langford, N. Megiddo. An Improved Predictive Accuracy Bound for Averaging Classifiers. /International Conference on Machine Learning 18: 290-297 <a href="https://infoscience.epfl.ch/record/161321/files/averaging_icml.pdf">[pdf]</a></p>
</li>
<li><p>(2000) C. Williams, M. Seeger. The Effect of the Input Density Distribution on Kernel-based Classifiers. <i>International Conference on Machine Learning 17: 1159-1166</i>. <a href="https://infoscience.epfl.ch/record/161323?ln=en">[link]</a></p>
</li>
<li><p>(2000) M. Seeger. Bayesian Model Selection for Support Vector Machines, Gaussian Processes and Other Kernel Classifiers. <i>Neural Information Processing Systems 13: 603-609</i>. <a href="https://papers.nips.cc/paper/1722-bayesian-model-selection-for-support-vector-machines-gaussian-processes-and-other-kernel-classifiers.pdf">[pdf]</a></p>
</li>
</ul>
<p><b>Journal:</b></p>
<ul>
<li><p>(2012) N. Srinivas, A. Krause, S. Kakade, M. Seeger. Information-Theoretic Regret Bounds for Gaussian Process Optimization in the Bandit Setting. <i>IEEE Transactions on Information Theory, 58: 3250-3265</i>. <a href="https://infoscience.epfl.ch/record/177246/files/srinivas_ieeeit2012.pdf">[pdf]</a></p>
</li>
<li><p>(2011) M. Seeger, H. Nickisch. Large Scale Bayesian Inference and Experimental Design for Sparse Linear Models. <i>SIAM Journal on Imaging Sciences, 4(1): 166-199</i>. <a href="https://infoscience.epfl.ch/record/164038/files/siims_finalpaper.pdf">[pdf]</a></p>
</li>
<li><p>(2010) M. Seeger, D. Wipf. Variational Bayesian Inference Techniques. <i>IEEE Signal Processing Magazine, 27(6): 81-91</i>. <a href="https://infoscience.epfl.ch/record/161294/files/final_paper.pdf">[pdf]</a></p>
</li>
<li><p>(2010) M. Seeger, H. Nickisch, R. Pohmann, B. Schoelkopf. Optimization of k-Space Trajectories for Compressed Sensing by Bayesian Experimental Design. <i>Magnetic Resonance in Medicine, 61(1): 116-126</i>. <a href="https://www.ncbi.nlm.nih.gov/pubmed/19859957">[PubMed]</a></p>
</li>
<li><p>(2008) M. Seeger. Cross-Validation Optimization for Large Scale Structured Classification Kernel Methods. <i>Journal of Machine Learning Research, 9: 1147-1178</i>. <a href="http://www.jmlr.org/papers/volume9/seeger08b/seeger08b.pdf">[pdf]</a></p>
</li>
<li><p>(2008) M. Seeger. Bayesian Inference and Optimal Design in the Sparse Linear Model. <i>Journal of Machine Learning Research, 9: 759-813</i>. <a href="http://www.jmlr.org/papers/volume9/seeger08a/seeger08a.pdf">[pdf]</a></p>
</li>
<li><p>(2008) M. Seeger, S. Kakade, D. Foster. Information Consistency of Nonparametric Gaussian Process Methods. <i>IEEE Transactions on Information Theory, 54(5): 2376-2382</i>. <a href="https://infoscience.epfl.ch/record/161300/files/infcons_ieeeit08.pdf">[pdf]</a></p>
</li>
<li><p>(2007) F. Steinke, M. Seeger, K. Tsuda. Experimental Design for Efficient Identification of Gene Regulatory Networks using Sparse Bayesian Models. <i>BMC Systems Biology, 1(51)</i> <a href="https://infoscience.epfl.ch/record/161460/files/bmcpaper.pdf">[pdf]</a></p>
</li>
<li><p>(2004) M. Seeger. Gaussian Processes for Machine Learning. <i>International Journal of Neural Systems, 14(2): 69-106</i>. <a href="https://infoscience.epfl.ch/record/161301/files/bayesgp-tut.pdf">[pdf]</a></p>
</li>
<li><p>(2002) M. Seeger. PAC-Bayesian Generalization Error Bounds for Gaussian Process Classification. <i>Journal of Machine Learning Research, 3: 233-269</i>. <a href="http://www.jmlr.org/papers/volume3/seeger02a/seeger02a.pdf">[pdf]</a></p>
</li>
</ul>
<p><b>Book Chapters:</b></p>
<ul>
<li><p>(2007) M. Seeger. Gaussian Process Belief Propagation. In <i>G. Bakir, T. Hofmann, B. Schoelkopf (eds.); Predicting Structured Data: 301-318</i>  <a href="https://infoscience.epfl.ch/record/161325/files/gpbp.pdf">[pdf]</a></p>
</li>
<li><p>(2006) M. Seeger. A Taxonomy for Semi-Supervised Learning Methods. In <i>O. Chapelle, B. Schoelkopf, A. Zien (eds.); Semi-Supervised Learning: 15-32</i> <a href="https://infoscience.epfl.ch/record/161326/files/chapter2.pdf">[pdf]</a></p>
</li>
</ul>
<p><b>Technical Reports:</b></p>
<ul>
<li><p>(2017) M. Seeger, S. Rangapuram, Y. Wang, D. Salinas, J. Gasthaus, T. Januschowski, V. Flunkert. Approximate Bayesian Inference in Linear State Space Models for Intermittent Demand Forecasting at Scale. <a href="https://arxiv.org/abs/1709.07638">[arxiv]</a></p>
</li>
<li><p>(2017) M. Seeger, A. Hetzel, Z. Dai, E. Meissner, N. Lawrence. Auto-Differentiating Linear Algebra. <a href="https://arxiv.org/abs/1710.08717">[arxiv]</a></p>
</li>
<li><p>(2010) N. Srinivas, A. Krause, S. Kakade, M. Seeger. Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design. <a href="https://arxiv.org/abs/0912.3995">[arxiv]</a></p>
</li>
<li><p>(2010) M. Seeger, H. Nickisch. Large Scale Bayesian Inference and Experimental Design for Sparse Linear Models. <a href="https://arxiv.org/abs/0810.0901">[arxiv]</a></p>
</li>
<li><p>(2005) M. Seeger, Y.-W. Teh, M. Jordan: Semiparametric Latent Factor Models. <a href="https://infoscience.epfl.ch/record/161465/files/slfm-long.pdf">[pdf]</a></p>
</li>
<li><p>(2005) M. Seeger. Expectation Propagation for Exponential Families. <a href="https://infoscience.epfl.ch/record/161464/files/epexpfam.pdf">[pdf]</a></p>
</li>
<li><p>(2004) M. Seeger. Low Rank Updates for the Cholesky Decomposition. <a href="https://infoscience.epfl.ch/record/161468/files/cholupdate.pdf">[pdf]</a></p>
</li>
<li><p>(2004) M. Seeger, M. Jordan. Sparse Gaussian Process Classification With Multiple Classes. <a href="https://infoscience.epfl.ch/record/161467/files/ivmmulti.pdf">[pdf]</a></p>
</li>
<li><p>(2000) M. Seeger. Learning with Labeled and Unlabeled Data. <a href="https://infoscience.epfl.ch/record/161327/files/review.pdf">[pdf]</a></p>
</li>
</ul>
<p><b>PhD Theses:</b></p>
<ul>
<li><p>(2017) Y. J. Ko. Applications of Approximate Learning and Inference for Probabilistic Models. <i>Ecole Polytechnique Federale, Lausanne (M. Grossglauser, M. Seeger, advisors)</i>. <a href="https://infoscience.epfl.ch/record/227482?ln=en">[link]</a></p>
</li>
<li><p>(2003) M. Seeger. Bayesian Gaussian Process Models: PAC-Bayesian Generalisation Error Bounds and Sparse Approximations. <i>University of Edinburgh, UK (C. Williams, advisor)</i>. <a href="https://infoscience.epfl.ch/record/161461?ln=en">[link]</a></p>
</li>
</ul>
</div></div>
<div class="infoblock">
<div class="blocktitle">Lecture Notes</div>
<div class="blockcontent">
<ul>
<li><p>(2012) Pattern Classification and Machine Learning, taught at EPFL <a href="files/pcml_notes.pdf">[pdf]</a></p>
</li>
</ul>
</div></div>
<div id="footer">
<div id="footer-text">
Page generated 2019-05-05 00:12:30 CEST, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</div>
</body>
</html>
